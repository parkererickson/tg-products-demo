{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e8ffad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tgml.data import TigerGraph\n",
    "\n",
    "tgraph = TigerGraph(\n",
    "    host=\"http://18.222.126.26\", # Replace with your instance ip\n",
    "    graph=\"OGBNProducts\",\n",
    "    username=\"tigergraph\",\n",
    "    password=\"tigergraph\",\n",
    "    token_auth=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ef42bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using graph 'OGBNProducts'\n",
      "---- Graph OGBNProducts\n",
      "Vertex Types: \n",
      "  - VERTEX Product(PRIMARY_ID id INT, x LIST<DOUBLE>, y INT, train_mask BOOL, val_mask BOOL, test_mask BOOL, tmp_id INT, tmp_id2 INT, tmp_id3 INT) WITH STATS=\"OUTDEGREE_BY_EDGETYPE\", PRIMARY_ID_AS_ATTRIBUTE=\"true\"\n",
      "Edge Types: \n",
      "  - UNDIRECTED EDGE Purchased(FROM Product, TO Product)\n",
      "\n",
      "Graphs: \n",
      "  - Graph OGBNProducts(Product:v, Purchased:e)\n",
      "Jobs: \n",
      "Queries: \n",
      "  - get_vertex_number(string v_type, string filter_by) (installed v2)\n",
      "  - shuffle_vertices(string tmp_id) (installed v2)\n",
      "  - tg_neighbor_sampler_x_y_train_mask_val_mask_test_mask(string vertex_filename, string edge_filename, int batch_id, int num_batches, int num_neighbors, int num_hops, string filter_by, string tmp_id) (installed v2)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tgraph.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13030bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2449029"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgraph.number_of_vertices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb989a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61859139"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgraph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88663e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices in training set: 196615\n",
      "Number of vertices in validation set: 39323\n",
      "Number of vertices in test set: 2213091\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Number of vertices in training set:\",\n",
    "    tgraph.number_of_vertices(filter_by=\"train_mask\"),\n",
    ")\n",
    "print(\n",
    "    \"Number of vertices in validation set:\",\n",
    "    tgraph.number_of_vertices(filter_by=\"val_mask\"),\n",
    ")\n",
    "print(\n",
    "    \"Number of vertices in test set:\", tgraph.number_of_vertices(filter_by=\"test_mask\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee08ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "    \"batch_size\": 1024,\n",
    "    \"num_neighbors\": 20,\n",
    "    \"num_hops\": 2,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"lr\":0.01,\n",
    "    \"l2_penalty\":0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19b5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tgml.dataloaders import NeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54732bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = NeighborLoader(\n",
    "    graph=tgraph,\n",
    "    tmp_id=\"tmp_id\",\n",
    "    v_in_feats=\"x\",\n",
    "    v_out_labels=\"y:int\",\n",
    "    v_extra_feats=\"train_mask:bool,val_mask:bool,test_mask:bool\",\n",
    "    output_format=\"PyG\",\n",
    "    batch_size=hp[\"batch_size\"],\n",
    "    num_neighbors=hp[\"num_neighbors\"],\n",
    "    num_hops=hp[\"num_hops\"],\n",
    "    shuffle=True,\n",
    "    filter_by=\"train_mask\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d95eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = NeighborLoader(\n",
    "    graph=tgraph,\n",
    "    tmp_id=\"tmp_id2\",\n",
    "    v_in_feats=\"x\",\n",
    "    v_out_labels=\"y:int\",\n",
    "    v_extra_feats=\"train_mask:bool,val_mask:bool,test_mask:bool\",\n",
    "    output_format=\"PyG\",\n",
    "    batch_size=hp[\"batch_size\"],\n",
    "    num_neighbors=hp[\"num_neighbors\"],\n",
    "    num_hops=hp[\"num_hops\"],\n",
    "    shuffle=False,\n",
    "    filter_by=\"val_mask\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c9ca2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = NeighborLoader(\n",
    "    graph=tgraph,\n",
    "    tmp_id=\"tmp_id3\",\n",
    "    v_in_feats=\"x\",\n",
    "    v_out_labels=\"y:int\",\n",
    "    v_extra_feats=\"train_mask:bool,val_mask:bool,test_mask:bool\",\n",
    "    output_format=\"PyG\",\n",
    "    batch_size=hp[\"batch_size\"],\n",
    "    num_neighbors=hp[\"num_neighbors\"],\n",
    "    num_hops=hp[\"num_hops\"],\n",
    "    shuffle=False,\n",
    "    filter_by=\"test_mask\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c18a878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abf1d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GraphSAGE(\n",
    "    in_channels=100, # dimension of x feature vectors\n",
    "    hidden_channels=hp[\"hidden_dim\"],\n",
    "    num_layers=hp[\"num_layers\"],\n",
    "    out_channels=47,\n",
    "    dropout=hp[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=hp[\"lr\"], weight_decay=hp[\"l2_penalty\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663ece56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from tgml.metrics import Accumulator, Accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2bb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Batch 0, Loss 3.8985, Accuracy 0.0089\n",
      "Epoch 0, Train Batch 1, Loss 3.7180, Accuracy 0.0809\n",
      "Epoch 0, Train Batch 2, Loss 3.1002, Accuracy 0.2480\n",
      "Epoch 0, Train Batch 3, Loss 2.8102, Accuracy 0.3180\n",
      "Epoch 0, Train Batch 4, Loss 2.5880, Accuracy 0.3554\n",
      "Epoch 0, Train Batch 5, Loss 2.4245, Accuracy 0.3873\n",
      "Epoch 0, Train Batch 6, Loss 2.3049, Accuracy 0.4119\n",
      "Epoch 0, Train Batch 7, Loss 2.1939, Accuracy 0.4438\n",
      "Epoch 0, Train Batch 8, Loss 2.0948, Accuracy 0.4751\n",
      "Epoch 0, Train Batch 9, Loss 2.0261, Accuracy 0.4946\n",
      "Epoch 0, Train Batch 10, Loss 1.9456, Accuracy 0.5152\n",
      "Epoch 0, Train Batch 11, Loss 1.8806, Accuracy 0.5317\n",
      "Epoch 0, Train Batch 12, Loss 1.8342, Accuracy 0.5429\n",
      "Epoch 0, Train Batch 13, Loss 1.7811, Accuracy 0.5555\n",
      "Epoch 0, Train Batch 14, Loss 1.7376, Accuracy 0.5663\n",
      "Epoch 0, Train Batch 15, Loss 1.7014, Accuracy 0.5757\n",
      "Epoch 0, Train Batch 16, Loss 1.6696, Accuracy 0.5841\n",
      "Epoch 0, Train Batch 17, Loss 1.6354, Accuracy 0.5923\n",
      "Epoch 0, Train Batch 18, Loss 1.6029, Accuracy 0.6002\n",
      "Epoch 0, Train Batch 19, Loss 1.5774, Accuracy 0.6063\n",
      "Epoch 0, Train Batch 20, Loss 1.5531, Accuracy 0.6123\n",
      "Epoch 0, Train Batch 21, Loss 1.5340, Accuracy 0.6169\n",
      "Epoch 0, Train Batch 22, Loss 1.5127, Accuracy 0.6227\n",
      "Epoch 0, Train Batch 23, Loss 1.4938, Accuracy 0.6273\n",
      "Epoch 0, Train Batch 24, Loss 1.4756, Accuracy 0.6323\n",
      "Epoch 0, Train Batch 25, Loss 1.4558, Accuracy 0.6368\n",
      "Epoch 0, Train Batch 26, Loss 1.4438, Accuracy 0.6402\n",
      "Epoch 0, Train Batch 27, Loss 1.4279, Accuracy 0.6439\n",
      "Epoch 0, Train Batch 28, Loss 1.4104, Accuracy 0.6479\n",
      "Epoch 0, Train Batch 29, Loss 1.3976, Accuracy 0.6512\n",
      "Epoch 0, Train Batch 30, Loss 1.3862, Accuracy 0.6542\n",
      "Epoch 0, Train Batch 31, Loss 1.3754, Accuracy 0.6565\n",
      "Epoch 0, Train Batch 32, Loss 1.3662, Accuracy 0.6590\n",
      "Epoch 0, Train Batch 33, Loss 1.3530, Accuracy 0.6621\n",
      "Epoch 0, Train Batch 34, Loss 1.3425, Accuracy 0.6646\n",
      "Epoch 0, Train Batch 35, Loss 1.3318, Accuracy 0.6671\n",
      "Epoch 0, Train Batch 36, Loss 1.3223, Accuracy 0.6692\n",
      "Epoch 0, Train Batch 37, Loss 1.3156, Accuracy 0.6711\n",
      "Epoch 0, Train Batch 38, Loss 1.3060, Accuracy 0.6734\n",
      "Epoch 0, Train Batch 39, Loss 1.2963, Accuracy 0.6756\n",
      "Epoch 0, Train Batch 40, Loss 1.2879, Accuracy 0.6776\n",
      "Epoch 0, Train Batch 41, Loss 1.2792, Accuracy 0.6798\n",
      "Epoch 0, Train Batch 42, Loss 1.2715, Accuracy 0.6813\n",
      "Epoch 0, Train Batch 43, Loss 1.2657, Accuracy 0.6829\n",
      "Epoch 0, Train Batch 44, Loss 1.2569, Accuracy 0.6851\n",
      "Epoch 0, Train Batch 45, Loss 1.2485, Accuracy 0.6870\n",
      "Epoch 0, Train Batch 46, Loss 1.2406, Accuracy 0.6888\n",
      "Epoch 0, Train Batch 47, Loss 1.2335, Accuracy 0.6904\n",
      "Epoch 0, Train Batch 48, Loss 1.2275, Accuracy 0.6917\n",
      "Epoch 0, Train Batch 49, Loss 1.2210, Accuracy 0.6932\n",
      "Epoch 0, Train Batch 50, Loss 1.2143, Accuracy 0.6946\n",
      "Epoch 0, Train Batch 51, Loss 1.2080, Accuracy 0.6959\n",
      "Epoch 0, Train Batch 52, Loss 1.2017, Accuracy 0.6973\n",
      "Epoch 0, Train Batch 53, Loss 1.1958, Accuracy 0.6988\n",
      "Epoch 0, Train Batch 54, Loss 1.1892, Accuracy 0.7003\n",
      "Epoch 0, Train Batch 55, Loss 1.1833, Accuracy 0.7016\n",
      "Epoch 0, Train Batch 56, Loss 1.1774, Accuracy 0.7029\n",
      "Epoch 0, Train Batch 57, Loss 1.1723, Accuracy 0.7040\n",
      "Epoch 0, Train Batch 58, Loss 1.1672, Accuracy 0.7052\n",
      "Epoch 0, Train Batch 59, Loss 1.1616, Accuracy 0.7063\n",
      "Epoch 0, Train Batch 60, Loss 1.1568, Accuracy 0.7073\n",
      "Epoch 0, Train Batch 61, Loss 1.1519, Accuracy 0.7085\n",
      "Epoch 0, Train Batch 62, Loss 1.1467, Accuracy 0.7096\n",
      "Epoch 0, Train Batch 63, Loss 1.1411, Accuracy 0.7109\n",
      "Epoch 0, Train Batch 64, Loss 1.1365, Accuracy 0.7119\n",
      "Epoch 0, Train Batch 65, Loss 1.1319, Accuracy 0.7128\n",
      "Epoch 0, Train Batch 66, Loss 1.1271, Accuracy 0.7140\n",
      "Epoch 0, Train Batch 67, Loss 1.1229, Accuracy 0.7149\n",
      "Epoch 0, Train Batch 68, Loss 1.1192, Accuracy 0.7159\n",
      "Epoch 0, Train Batch 69, Loss 1.1151, Accuracy 0.7168\n",
      "Epoch 0, Train Batch 70, Loss 1.1120, Accuracy 0.7174\n",
      "Epoch 0, Train Batch 71, Loss 1.1077, Accuracy 0.7185\n",
      "Epoch 0, Train Batch 72, Loss 1.1031, Accuracy 0.7195\n",
      "Epoch 0, Train Batch 73, Loss 1.0999, Accuracy 0.7201\n",
      "Epoch 0, Train Batch 74, Loss 1.0963, Accuracy 0.7208\n",
      "Epoch 0, Train Batch 75, Loss 1.0929, Accuracy 0.7215\n",
      "Epoch 0, Train Batch 76, Loss 1.0891, Accuracy 0.7225\n",
      "Epoch 0, Train Batch 77, Loss 1.0851, Accuracy 0.7234\n",
      "Epoch 0, Train Batch 78, Loss 1.0818, Accuracy 0.7242\n",
      "Epoch 0, Train Batch 79, Loss 1.0783, Accuracy 0.7250\n",
      "Epoch 0, Train Batch 80, Loss 1.0750, Accuracy 0.7257\n",
      "Epoch 0, Train Batch 81, Loss 1.0715, Accuracy 0.7266\n",
      "Epoch 0, Train Batch 82, Loss 1.0681, Accuracy 0.7274\n",
      "Epoch 0, Train Batch 83, Loss 1.0648, Accuracy 0.7281\n",
      "Epoch 0, Train Batch 84, Loss 1.0614, Accuracy 0.7289\n",
      "Epoch 0, Train Batch 85, Loss 1.0588, Accuracy 0.7295\n",
      "Epoch 0, Train Batch 86, Loss 1.0553, Accuracy 0.7301\n",
      "Epoch 0, Train Batch 87, Loss 1.0526, Accuracy 0.7308\n",
      "Epoch 0, Train Batch 88, Loss 1.0497, Accuracy 0.7315\n",
      "Epoch 0, Train Batch 89, Loss 1.0472, Accuracy 0.7322\n",
      "Epoch 0, Train Batch 90, Loss 1.0445, Accuracy 0.7328\n",
      "Epoch 0, Train Batch 91, Loss 1.0419, Accuracy 0.7334\n",
      "Epoch 0, Train Batch 92, Loss 1.0395, Accuracy 0.7340\n",
      "Epoch 0, Train Batch 93, Loss 1.0362, Accuracy 0.7347\n",
      "Epoch 0, Train Batch 94, Loss 1.0339, Accuracy 0.7352\n",
      "Epoch 0, Train Batch 95, Loss 1.0314, Accuracy 0.7358\n",
      "Epoch 0, Train Batch 96, Loss 1.0287, Accuracy 0.7364\n",
      "Epoch 0, Train Batch 97, Loss 1.0256, Accuracy 0.7372\n",
      "Epoch 0, Train Batch 98, Loss 1.0235, Accuracy 0.7377\n",
      "Epoch 0, Train Batch 99, Loss 1.0209, Accuracy 0.7383\n",
      "Epoch 0, Train Batch 100, Loss 1.0188, Accuracy 0.7388\n",
      "Epoch 0, Train Batch 101, Loss 1.0171, Accuracy 0.7392\n",
      "Epoch 0, Train Batch 102, Loss 1.0146, Accuracy 0.7398\n",
      "Epoch 0, Train Batch 103, Loss 1.0121, Accuracy 0.7404\n",
      "Epoch 0, Train Batch 104, Loss 1.0099, Accuracy 0.7409\n",
      "Epoch 0, Train Batch 105, Loss 1.0083, Accuracy 0.7413\n",
      "Epoch 0, Train Batch 106, Loss 1.0056, Accuracy 0.7420\n",
      "Epoch 0, Train Batch 107, Loss 1.0034, Accuracy 0.7425\n",
      "Epoch 0, Train Batch 108, Loss 1.0011, Accuracy 0.7429\n",
      "Epoch 0, Train Batch 109, Loss 0.9985, Accuracy 0.7435\n",
      "Epoch 0, Train Batch 110, Loss 0.9962, Accuracy 0.7441\n",
      "Epoch 0, Train Batch 111, Loss 0.9943, Accuracy 0.7445\n",
      "Epoch 0, Train Batch 112, Loss 0.9922, Accuracy 0.7450\n",
      "Epoch 0, Train Batch 113, Loss 0.9910, Accuracy 0.7453\n",
      "Epoch 0, Train Batch 114, Loss 0.9889, Accuracy 0.7458\n",
      "Epoch 0, Train Batch 115, Loss 0.9865, Accuracy 0.7463\n",
      "Epoch 0, Train Batch 116, Loss 0.9853, Accuracy 0.7466\n",
      "Epoch 0, Train Batch 117, Loss 0.9835, Accuracy 0.7469\n",
      "Epoch 0, Train Batch 118, Loss 0.9817, Accuracy 0.7473\n",
      "Epoch 0, Train Batch 119, Loss 0.9797, Accuracy 0.7478\n",
      "Epoch 0, Train Batch 120, Loss 0.9776, Accuracy 0.7483\n",
      "Epoch 0, Train Batch 121, Loss 0.9762, Accuracy 0.7487\n",
      "Epoch 0, Train Batch 122, Loss 0.9746, Accuracy 0.7491\n",
      "Epoch 0, Train Batch 123, Loss 0.9726, Accuracy 0.7496\n",
      "Epoch 0, Train Batch 124, Loss 0.9708, Accuracy 0.7500\n",
      "Epoch 0, Train Batch 125, Loss 0.9686, Accuracy 0.7504\n",
      "Epoch 0, Train Batch 126, Loss 0.9671, Accuracy 0.7507\n",
      "Epoch 0, Train Batch 127, Loss 0.9659, Accuracy 0.7511\n",
      "Epoch 0, Train Batch 128, Loss 0.9645, Accuracy 0.7515\n",
      "Epoch 0, Train Batch 129, Loss 0.9631, Accuracy 0.7518\n",
      "Epoch 0, Train Batch 130, Loss 0.9612, Accuracy 0.7522\n",
      "Epoch 0, Train Batch 131, Loss 0.9602, Accuracy 0.7525\n",
      "Epoch 0, Train Batch 132, Loss 0.9584, Accuracy 0.7529\n",
      "Epoch 0, Train Batch 133, Loss 0.9570, Accuracy 0.7532\n",
      "Epoch 0, Train Batch 134, Loss 0.9555, Accuracy 0.7535\n",
      "Epoch 0, Train Batch 135, Loss 0.9537, Accuracy 0.7539\n",
      "Epoch 0, Train Batch 136, Loss 0.9525, Accuracy 0.7541\n",
      "Epoch 0, Train Batch 137, Loss 0.9508, Accuracy 0.7545\n",
      "Epoch 0, Train Batch 138, Loss 0.9494, Accuracy 0.7549\n",
      "Epoch 0, Train Batch 139, Loss 0.9481, Accuracy 0.7552\n",
      "Epoch 0, Train Batch 140, Loss 0.9467, Accuracy 0.7554\n",
      "Epoch 0, Train Batch 141, Loss 0.9452, Accuracy 0.7558\n",
      "Epoch 0, Train Batch 142, Loss 0.9441, Accuracy 0.7560\n",
      "Epoch 0, Train Batch 143, Loss 0.9436, Accuracy 0.7563\n",
      "Epoch 0, Train Batch 144, Loss 0.9423, Accuracy 0.7566\n",
      "Epoch 0, Train Batch 145, Loss 0.9413, Accuracy 0.7569\n",
      "Epoch 0, Train Batch 146, Loss 0.9401, Accuracy 0.7572\n",
      "Epoch 0, Train Batch 147, Loss 0.9390, Accuracy 0.7575\n",
      "Epoch 0, Train Batch 148, Loss 0.9377, Accuracy 0.7577\n",
      "Epoch 0, Train Batch 149, Loss 0.9365, Accuracy 0.7580\n",
      "Epoch 0, Train Batch 150, Loss 0.9352, Accuracy 0.7583\n",
      "Epoch 0, Train Batch 151, Loss 0.9339, Accuracy 0.7585\n",
      "Epoch 0, Train Batch 152, Loss 0.9326, Accuracy 0.7588\n",
      "Epoch 0, Train Batch 153, Loss 0.9315, Accuracy 0.7591\n",
      "Epoch 0, Train Batch 154, Loss 0.9306, Accuracy 0.7593\n",
      "Epoch 0, Train Batch 155, Loss 0.9293, Accuracy 0.7596\n",
      "Epoch 0, Train Batch 156, Loss 0.9286, Accuracy 0.7598\n",
      "Epoch 0, Train Batch 157, Loss 0.9276, Accuracy 0.7601\n",
      "Epoch 0, Train Batch 158, Loss 0.9262, Accuracy 0.7603\n",
      "Epoch 0, Train Batch 159, Loss 0.9255, Accuracy 0.7605\n",
      "Epoch 0, Train Batch 160, Loss 0.9244, Accuracy 0.7608\n",
      "Epoch 0, Train Batch 161, Loss 0.9229, Accuracy 0.7611\n",
      "Epoch 0, Train Batch 162, Loss 0.9221, Accuracy 0.7613\n",
      "Epoch 0, Train Batch 163, Loss 0.9209, Accuracy 0.7616\n",
      "Epoch 0, Train Batch 164, Loss 0.9198, Accuracy 0.7618\n",
      "Epoch 0, Train Batch 165, Loss 0.9186, Accuracy 0.7621\n",
      "Epoch 0, Train Batch 166, Loss 0.9178, Accuracy 0.7623\n",
      "Epoch 0, Train Batch 167, Loss 0.9170, Accuracy 0.7624\n",
      "Epoch 0, Train Batch 168, Loss 0.9161, Accuracy 0.7627\n",
      "Epoch 0, Train Batch 169, Loss 0.9152, Accuracy 0.7629\n",
      "Epoch 0, Train Batch 170, Loss 0.9140, Accuracy 0.7632\n",
      "Epoch 0, Train Batch 171, Loss 0.9127, Accuracy 0.7634\n",
      "Epoch 0, Train Batch 172, Loss 0.9120, Accuracy 0.7637\n",
      "Epoch 0, Train Batch 173, Loss 0.9108, Accuracy 0.7640\n",
      "Epoch 0, Train Batch 174, Loss 0.9094, Accuracy 0.7643\n",
      "Epoch 0, Train Batch 175, Loss 0.9084, Accuracy 0.7646\n",
      "Epoch 0, Train Batch 176, Loss 0.9073, Accuracy 0.7648\n",
      "Epoch 0, Train Batch 177, Loss 0.9062, Accuracy 0.7651\n",
      "Epoch 0, Train Batch 178, Loss 0.9058, Accuracy 0.7653\n",
      "Epoch 0, Train Batch 179, Loss 0.9051, Accuracy 0.7655\n",
      "Epoch 0, Train Batch 180, Loss 0.9044, Accuracy 0.7657\n",
      "Epoch 0, Train Batch 181, Loss 0.9034, Accuracy 0.7659\n",
      "Epoch 0, Train Batch 182, Loss 0.9027, Accuracy 0.7660\n",
      "Epoch 0, Train Batch 183, Loss 0.9020, Accuracy 0.7662\n",
      "Epoch 0, Train Batch 184, Loss 0.9010, Accuracy 0.7664\n",
      "Epoch 0, Train Batch 185, Loss 0.9002, Accuracy 0.7666\n",
      "Epoch 0, Train Batch 186, Loss 0.8994, Accuracy 0.7668\n",
      "Epoch 0, Train Batch 187, Loss 0.8986, Accuracy 0.7669\n",
      "Epoch 0, Train Batch 188, Loss 0.8979, Accuracy 0.7671\n",
      "Epoch 0, Train Batch 189, Loss 0.8970, Accuracy 0.7673\n",
      "Epoch 0, Train Batch 190, Loss 0.8962, Accuracy 0.7675\n",
      "Epoch 0, Train Batch 191, Loss 0.8956, Accuracy 0.7676\n",
      "Epoch 0, Train Batch 192, Loss 0.8948, Accuracy 0.7678\n",
      "Epoch 0, Valid Loss 0.9891, Valid Accuracy 0.7371\n",
      "Epoch 1, Train Batch 0, Loss 0.7694, Accuracy 0.8006\n",
      "Epoch 1, Train Batch 1, Loss 0.7551, Accuracy 0.7989\n",
      "Epoch 1, Train Batch 2, Loss 0.7564, Accuracy 0.7988\n",
      "Epoch 1, Train Batch 3, Loss 0.7479, Accuracy 0.8016\n",
      "Epoch 1, Train Batch 4, Loss 0.7460, Accuracy 0.8021\n",
      "Epoch 1, Train Batch 5, Loss 0.7450, Accuracy 0.8026\n",
      "Epoch 1, Train Batch 6, Loss 0.7513, Accuracy 0.8014\n",
      "Epoch 1, Train Batch 7, Loss 0.7465, Accuracy 0.8026\n",
      "Epoch 1, Train Batch 8, Loss 0.7510, Accuracy 0.8003\n",
      "Epoch 1, Train Batch 9, Loss 0.7477, Accuracy 0.8005\n",
      "Epoch 1, Train Batch 10, Loss 0.7476, Accuracy 0.8008\n",
      "Epoch 1, Train Batch 11, Loss 0.7468, Accuracy 0.8012\n",
      "Epoch 1, Train Batch 12, Loss 0.7455, Accuracy 0.8012\n",
      "Epoch 1, Train Batch 13, Loss 0.7426, Accuracy 0.8021\n",
      "Epoch 1, Train Batch 14, Loss 0.7452, Accuracy 0.8014\n",
      "Epoch 1, Train Batch 15, Loss 0.7450, Accuracy 0.8019\n",
      "Epoch 1, Train Batch 16, Loss 0.7407, Accuracy 0.8027\n",
      "Epoch 1, Train Batch 17, Loss 0.7385, Accuracy 0.8035\n",
      "Epoch 1, Train Batch 18, Loss 0.7403, Accuracy 0.8037\n",
      "Epoch 1, Train Batch 19, Loss 0.7398, Accuracy 0.8037\n",
      "Epoch 1, Train Batch 20, Loss 0.7413, Accuracy 0.8032\n",
      "Epoch 1, Train Batch 21, Loss 0.7404, Accuracy 0.8036\n",
      "Epoch 1, Train Batch 22, Loss 0.7384, Accuracy 0.8040\n",
      "Epoch 1, Train Batch 23, Loss 0.7389, Accuracy 0.8041\n",
      "Epoch 1, Train Batch 24, Loss 0.7384, Accuracy 0.8041\n",
      "Epoch 1, Train Batch 25, Loss 0.7366, Accuracy 0.8044\n",
      "Epoch 1, Train Batch 26, Loss 0.7375, Accuracy 0.8043\n",
      "Epoch 1, Train Batch 27, Loss 0.7367, Accuracy 0.8044\n",
      "Epoch 1, Train Batch 28, Loss 0.7354, Accuracy 0.8046\n",
      "Epoch 1, Train Batch 29, Loss 0.7341, Accuracy 0.8048\n",
      "Epoch 1, Train Batch 30, Loss 0.7347, Accuracy 0.8046\n",
      "Epoch 1, Train Batch 31, Loss 0.7317, Accuracy 0.8055\n",
      "Epoch 1, Train Batch 32, Loss 0.7325, Accuracy 0.8055\n",
      "Epoch 1, Train Batch 33, Loss 0.7316, Accuracy 0.8057\n",
      "Epoch 1, Train Batch 34, Loss 0.7306, Accuracy 0.8058\n",
      "Epoch 1, Train Batch 35, Loss 0.7317, Accuracy 0.8057\n",
      "Epoch 1, Train Batch 36, Loss 0.7312, Accuracy 0.8056\n",
      "Epoch 1, Train Batch 37, Loss 0.7302, Accuracy 0.8059\n",
      "Epoch 1, Train Batch 38, Loss 0.7299, Accuracy 0.8061\n",
      "Epoch 1, Train Batch 39, Loss 0.7297, Accuracy 0.8060\n",
      "Epoch 1, Train Batch 40, Loss 0.7294, Accuracy 0.8061\n",
      "Epoch 1, Train Batch 41, Loss 0.7294, Accuracy 0.8060\n",
      "Epoch 1, Train Batch 42, Loss 0.7283, Accuracy 0.8062\n",
      "Epoch 1, Train Batch 43, Loss 0.7281, Accuracy 0.8063\n",
      "Epoch 1, Train Batch 44, Loss 0.7281, Accuracy 0.8065\n",
      "Epoch 1, Train Batch 45, Loss 0.7279, Accuracy 0.8064\n",
      "Epoch 1, Train Batch 46, Loss 0.7280, Accuracy 0.8063\n",
      "Epoch 1, Train Batch 47, Loss 0.7281, Accuracy 0.8064\n",
      "Epoch 1, Train Batch 48, Loss 0.7276, Accuracy 0.8065\n",
      "Epoch 1, Train Batch 49, Loss 0.7271, Accuracy 0.8067\n",
      "Epoch 1, Train Batch 50, Loss 0.7264, Accuracy 0.8068\n",
      "Epoch 1, Train Batch 51, Loss 0.7274, Accuracy 0.8065\n",
      "Epoch 1, Train Batch 52, Loss 0.7272, Accuracy 0.8066\n",
      "Epoch 1, Train Batch 53, Loss 0.7260, Accuracy 0.8068\n",
      "Epoch 1, Train Batch 54, Loss 0.7251, Accuracy 0.8071\n",
      "Epoch 1, Train Batch 55, Loss 0.7243, Accuracy 0.8073\n",
      "Epoch 1, Train Batch 56, Loss 0.7241, Accuracy 0.8074\n",
      "Epoch 1, Train Batch 57, Loss 0.7236, Accuracy 0.8074\n",
      "Epoch 1, Train Batch 58, Loss 0.7242, Accuracy 0.8075\n",
      "Epoch 1, Train Batch 59, Loss 0.7235, Accuracy 0.8077\n",
      "Epoch 1, Train Batch 60, Loss 0.7238, Accuracy 0.8077\n",
      "Epoch 1, Train Batch 61, Loss 0.7238, Accuracy 0.8077\n",
      "Epoch 1, Train Batch 62, Loss 0.7236, Accuracy 0.8078\n",
      "Epoch 1, Train Batch 63, Loss 0.7229, Accuracy 0.8080\n",
      "Epoch 1, Train Batch 64, Loss 0.7230, Accuracy 0.8081\n",
      "Epoch 1, Train Batch 65, Loss 0.7228, Accuracy 0.8081\n",
      "Epoch 1, Train Batch 66, Loss 0.7222, Accuracy 0.8082\n",
      "Epoch 1, Train Batch 67, Loss 0.7222, Accuracy 0.8082\n",
      "Epoch 1, Train Batch 68, Loss 0.7224, Accuracy 0.8080\n",
      "Epoch 1, Train Batch 69, Loss 0.7229, Accuracy 0.8080\n",
      "Epoch 1, Train Batch 70, Loss 0.7225, Accuracy 0.8081\n",
      "Epoch 1, Train Batch 71, Loss 0.7220, Accuracy 0.8083\n",
      "Epoch 1, Train Batch 72, Loss 0.7219, Accuracy 0.8082\n",
      "Epoch 1, Train Batch 73, Loss 0.7210, Accuracy 0.8084\n",
      "Epoch 1, Train Batch 74, Loss 0.7207, Accuracy 0.8085\n",
      "Epoch 1, Train Batch 75, Loss 0.7208, Accuracy 0.8086\n",
      "Epoch 1, Train Batch 76, Loss 0.7208, Accuracy 0.8085\n",
      "Epoch 1, Train Batch 77, Loss 0.7206, Accuracy 0.8085\n",
      "Epoch 1, Train Batch 78, Loss 0.7211, Accuracy 0.8084\n",
      "Epoch 1, Train Batch 79, Loss 0.7205, Accuracy 0.8085\n",
      "Epoch 1, Train Batch 80, Loss 0.7199, Accuracy 0.8086\n",
      "Epoch 1, Train Batch 81, Loss 0.7201, Accuracy 0.8088\n",
      "Epoch 1, Train Batch 82, Loss 0.7195, Accuracy 0.8090\n",
      "Epoch 1, Train Batch 83, Loss 0.7194, Accuracy 0.8090\n",
      "Epoch 1, Train Batch 84, Loss 0.7192, Accuracy 0.8090\n",
      "Epoch 1, Train Batch 85, Loss 0.7185, Accuracy 0.8092\n",
      "Epoch 1, Train Batch 86, Loss 0.7178, Accuracy 0.8094\n",
      "Epoch 1, Train Batch 87, Loss 0.7174, Accuracy 0.8094\n",
      "Epoch 1, Train Batch 88, Loss 0.7169, Accuracy 0.8095\n",
      "Epoch 1, Train Batch 89, Loss 0.7177, Accuracy 0.8093\n",
      "Epoch 1, Train Batch 90, Loss 0.7177, Accuracy 0.8092\n",
      "Epoch 2, Train Batch 35, Loss 0.6789, Accuracy 0.8193\n",
      "Epoch 2, Train Batch 36, Loss 0.6790, Accuracy 0.8191\n",
      "Epoch 2, Train Batch 37, Loss 0.6777, Accuracy 0.8194\n",
      "Epoch 2, Train Batch 138, Loss 0.6822, Accuracy 0.8187\n",
      "Epoch 2, Train Batch 139, Loss 0.6824, Accuracy 0.8187\n",
      "Epoch 2, Train Batch 140, Loss 0.6825, Accuracy 0.8188\n",
      "Epoch 2, Train Batch 141, Loss 0.6821, Accuracy 0.8188\n",
      "Epoch 2, Train Batch 142, Loss 0.6818, Accuracy 0.8189\n",
      "Epoch 2, Train Batch 143, Loss 0.6816, Accuracy 0.8190\n",
      "Epoch 2, Train Batch 144, Loss 0.6816, Accuracy 0.8189\n"
     ]
    }
   ],
   "source": [
    "log_dir = \"logs/products/graphsage/subgraph/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log = SummaryWriter(log_dir+\"/train\")\n",
    "valid_log = SummaryWriter(log_dir+\"/valid\")\n",
    "global_steps = 0\n",
    "logs = {}\n",
    "for epoch in range(10):\n",
    "    # Train\n",
    "    model.train()\n",
    "    epoch_train_loss = Accumulator()\n",
    "    epoch_train_acc = Accuracy()\n",
    "    for bid, batch in enumerate(train_loader):\n",
    "        batchsize = batch.x.shape[0]\n",
    "        batch.to(device)\n",
    "        # Forward pass\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss.update(loss.item() * batchsize, batchsize)\n",
    "        # Predict on training data\n",
    "        with torch.no_grad():\n",
    "            pred = out.argmax(dim=1)\n",
    "            epoch_train_acc.update(pred[batch.train_mask], batch.y[batch.train_mask])\n",
    "        # Log training status after each batch\n",
    "        logs[\"loss\"] = epoch_train_loss.mean\n",
    "        logs[\"acc\"] = epoch_train_acc.value\n",
    "        print(\n",
    "            \"Epoch {}, Train Batch {}, Loss {:.4f}, Accuracy {:.4f}\".format(\n",
    "                epoch, bid, logs[\"loss\"], logs[\"acc\"]\n",
    "            )\n",
    "        )\n",
    "        train_log.add_scalar(\"Loss\", logs[\"loss\"], global_steps)\n",
    "        train_log.add_scalar(\"Accuracy\", logs[\"acc\"], global_steps)\n",
    "        train_log.flush()\n",
    "        global_steps += 1\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    epoch_val_loss = Accumulator()\n",
    "    epoch_val_acc = Accuracy()\n",
    "    for batch in valid_loader:\n",
    "        batchsize = batch.x.shape[0]\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            # Calculate loss\n",
    "            valid_loss = F.cross_entropy(out[batch.val_mask], batch.y[batch.val_mask])\n",
    "            epoch_val_loss.update(valid_loss.item() * batchsize, batchsize)\n",
    "            # Prediction\n",
    "            pred = out.argmax(dim=1)\n",
    "            epoch_val_acc.update(pred[batch.val_mask], batch.y[batch.val_mask])\n",
    "    # Log testing result after each epoch\n",
    "    logs[\"val_loss\"] = epoch_val_loss.mean\n",
    "    logs[\"val_acc\"] = epoch_val_acc.value\n",
    "    print(\n",
    "        \"Epoch {}, Valid Loss {:.4f}, Valid Accuracy {:.4f}\".format(\n",
    "            epoch, logs[\"val_loss\"], logs[\"val_acc\"]\n",
    "        )\n",
    "    )\n",
    "    valid_log.add_scalar(\"Loss\", logs[\"val_loss\"], global_steps)\n",
    "    valid_log.add_scalar(\"Accuracy\", logs[\"val_acc\"], global_steps)\n",
    "    valid_log.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "acc = Accuracy()\n",
    "for batch in test_loader:\n",
    "    batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(batch.x, batch.edge_index).argmax(dim=1)\n",
    "        acc.update(pred[batch.test_mask], batch.y[batch.test_mask])\n",
    "print(\"Accuracy: {:.4f}\".format(acc.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37dcab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TigerGraph-ML-PyG",
   "language": "python",
   "name": "tigergraph-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
